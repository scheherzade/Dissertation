\section{Analytical Modeling}
In the previous section we studied the possibility of using a polynomial to capture the relationship between grain size, number of cores, and throughput for a fixed matrix size, with the purpose of finding a range of grain size that leads us to maximum performance. 
Although the polynomial function was helpful in directing us toward our objective of finding the region with maximum performance, it lacked a physical implication, and was not quite able to capture the overall behavior of the system. 

This motivated us to change our view, and instead of looking just at the data and trying to find a function to fit the data, study the behavior of the data, and then find a function that would be likely to fit and explain the data. That function would be a good fit mostly because that's how we expect the throughput to change with grain size, and not solely how the data behaves.   

In this chapter we attempt to understand the effect of grain size on the achievable speedup in an asynchronous many-task runtime system, in order to develop an analytical model for predicting the execution time in an asynchronous many task runtime system. 

We have to note here that even if we were able to identify all the factors affecting the execution time, it is still very hard to find an analytical model describing the relationship between these factors and the execution time. In this chapter we explain our effort in this direction by starting from a simple benchmark. We suggest a formula based on our knowledge of the behavior of the system, and the collected data. Then we will try to generalize the proposed formula to arbitrary for loops with balanced work-load for each iteration. 


Throughout this chapter we will be using the following terms and definitions to simplify our work. We represent the number of cores available with $N$, the number of tasks created as $num\_{tasks}$, the maximum amount of work assigned to one core as $w\_c$, the number of cores that are actually doing the work as $M$ ($M\leq{N}$), the total amount of work available as $problem\_{size}$, and the sequential execution time as $t\_{seq}$.

In an attempt to find this analytical model for execution time of a balanced parallel for loop, we started with looking into two factors that we believe play the most important role in determining the execution time, the overhead of creating tasks, and the maximum amount of work assigned to one core. 

In order to understand how these factors contribute to the execution time in a balanced parallel for loop, we define $iter\_{length}$ as the amount of time to execute one iteration, $num\_{iterations}$ as total number of iterations, $chunk\_{size}$ as number of iterations to be executed by one thread.


The total execution time could be assumed to be roughly the amount of time it takes for the core with the maximum amount of work to finish it's job. Here we call the core with maximum expected amount of work as $core_0$, and the associated amount of work as $w\_c$. 
With this assumption, the main factors contributing to the execution time for this simple problem are the overhead of creating tasks on $core_0$, the time it takes to run $w\_c$ amount of work on $core_0$, denoted with $t\_{c}$, and the number of cores that will be executing the work($M$). 


Formula~\ref{formula31} shows the expected formula in it's simplest form, with $q$ as a constant factor to account for unknown factors.  

\begin{equation}\label{formula31}
\begin{aligned}
&execution\_time = 
t\_{overhead}\:\:+\:\:t\_c\
\end{aligned}
\end{equation}


In Formula\ref{formula31}, $t\_{overhead}$ represents the penalty that we have to pay for running the program in parallel. We hold two major factors accountable for this overhead. 

The first factor is the overhead of creating the tasks. The overhead of creating one task itself is small, but as the number of created tasks becomes larger, it becomes significant.
When $num\_{tasks}$ tasks are created, $\left\lceil{\frac{num\_{tasks}}{N}}\right \rceil$ tasks would be created on $core\_0$. If we represent the overhead of creating a task on one core with $\alpha$, the portion of $t\_{overhead}$ resulted from creating tasks could be stated as $\alpha\times{\left\lceil{\frac{num\_{tasks}}{N}}\right \rceil}$.

%The second factor is the overhead of managing the created tasks. While the overhead of creating a task should be constant on a specific machine architecture, we are suggesting here that the overall overhead of managing $num\_{tasks}$ tasks, is a factor($\delta$) of $N$. This effect is originated from HPX's strategy to avoid allocating new stacks by keeping the previous stack alive until new thread is created, at that point the stack is assigned to the new thread. 
%The overhead of managing a task on a machine is shown with $\delta\times{num\_{tasks}}$.

The second factor is the overhead caused by work stealing. Although work stealing is a very helpful method to achieve balanced loads, it induces an overhead due to constant efforts of idle cores to steal work from the other cores. Each of these efforts would result in trashing the local cache of the busy cores. This effect helps for load balancing when the number of tasks created is greater than or equal to the number of cores, but as the number of tasks get smaller than the number of cores, the unsuccessful steal efforts become more noticeable.

HPX by default uses the priority local scheduling policy which creates one queue for each OS thread. When there is no more work left in on core's queue, it will attempt to steal work from other cores starting from it's neighbors. If the attempt is not successful, it will move to the next level neighbors. This continues until the core that initiates the steals(called the thief) is able to find a core with tasks in their queue which can can be stolen(called the victim). 

There is an optional command line option ${--hpx:numa-sensitive}$ to make sure that the thief would try the queues of the cores in the same NUMA domain first. This option is provided based on the fact that it is much faster to access the local memory of a processor than the local memory of another processor.  
%In our example we are only using 8 cores from the same NUMA domain 
With,
\begin{equation}\label{formula33}  
\begin{aligned}
t_{overhead}=\alpha\times{\left\lceil{\frac{num\_{tasks}}{N}}\right\rceil}\:\:+\:\:t\_{overhead\_{ws}}
\end{aligned}
\end{equation}

Formula~\ref{formula31} then becomes:

\begin{equation}\label{formula32}
\begin{aligned}
&execution\_time = 
\alpha\times{\left\lceil{\frac{num\_{tasks}}{N}}\right\rceil\:\:+\:\:t\_{overhead\_{ws}}\:\:+\:\:t\_c}
\end{aligned}
\end{equation}

$w\_c$ can be calculated as:
 
\begin{equation}\label{formula2}
w\_c =\left\{
\begin{aligned}
&problem\_{size} \:\:\:\:\:\:\:\:\:\:\:\:\text{ if } N=1\\
&problem\_{size}-g\times{(N-1)}\times(\left \lceil{\frac{num\_{tasks}}{N}}\right \rceil-1) \\
&\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\text{if }num\_{tasks}\%N=1\text{\:\:\&\:\: }num\_{iterations}\%chunk\_{size}\neq0\\ 
%\text{\:\:or} \\ g\times(\left \lceil{\frac{num\_{tasks}}{M}}\right \rceil-1)+(num\_{iterations}\%chunk\_{size})\times(iter\_{length}) \\ \:\:\:\:\:\:\:\:       
&g\times\left \lceil{\frac{num\_{tasks}}{N}}\right \rceil  \:\:\:\:\:\:\:\:\:\:    \text{otherwise}
\end{aligned}
\right.
\end{equation}

 
%On the other hand, when number of tasks created is smaller than the number of cores, we observe continuous efforts from the idle cores to steal work from cores that are actually doing the work, interrupting the cache coherency constantly. We represent this effect with $\delta\times{(N-M)\times{M}}\times{num\_{tasks}}\times{\frac{1}{M}})$.

  

%\begin{equation}\label{formula1}
%\begin{aligned}
%&execution\_time = \\
%\left\{
%\begin{aligned}
%&
%\alpha\left\lceil{\frac{num\_{tasks}}{M}}\right \rceil +\beta\times{(N-1)}{num\_{tasks}}+{w\_c+\gamma\times{w\_c}\times{(M-1)}} \:\: \text{if\:\:}num\_{tasks}>N\\
%&\alpha\left \lceil{\frac{num\_{tasks}}{M}}\right \rceil +\delta\times\frac{num\_{tasks}-1}{N}+{w\_c+\gamma\times{w\_c}\times{(M-1)}} \:\: \text{otherwise}
%\end{aligned}
%\right.
%\end{aligned}
%\end{equation}
%\vspace{\baselineskip}



As discussed in chapter~\ref{Background}, Amdahl's law and Universal Scalibility Law, suggest that, for a fixed problem size, as we increase the number of cores in a multicore system, we do not observe a linear speedup, which is mainly resulted from latency and coherency.
Universal Scalibilty Law\cite{gunther2007guerrilla} suggests that an overhead associated with the number of cores should be added to the expected execution time. We added a term to Formula~\ref{formula2} to represent this effect. Moreover, we adjusted the formula by changing the total number of cores ($N$) to the number of cores that are actually executing the work ($M$). 
Assuming we are running our application on $N$ cores, with a grain size equal to $g$, $num\_{tasks}$ tasks are being created, and $M$ cores are actually doing the work. If $num\_{tasks}<N$, $M$ would be equal to $num\_{tasks}$, otherwise $M=N$.

\begin{equation}\label{formula19}
M=\left\{
\begin{aligned}
&num\_{tasks} \text{\:\:\:\:if \:} num\_{tasks}<N\\
&N \text{\:\:\:\:otherwise}
\end{aligned}
\right.
\end{equation}


Formula~\ref{formula2} is then changed into:

\begin{equation}\label{formula1}
\begin{aligned}
execution\_time = 
\alpha\times{\left\lceil{\frac{num\_{tasks}}{N}}\right\rceil}\:\:&+\:\:t\_c\:\:+\:\:\gamma\times{t\_c}\times{(M-1)}\:\:\\
&+\:\:+\:\:t\_{overhead\_{ws}}
\end{aligned}
\end{equation}

Here $t\_{overhead\_{ws}}$ represents all the overhead created due to work stealing. We did some investigations on where this term is originated from for a more precise modeling, but we were not successful. In order to study this effect, we ran a set of experiments while work stealing was turned of. This is possible through adding the command line option \emph{--hpx:queuing = static-priority}.

As for $t\_{c}$, the time to execute $w\_{c}$ amount of work, where total amount of work available is $problem\_{size}$, since we are looking into balanced loops in this problem, we can estimate $t\_{c}$ as:

\begin{equation}\label{formula41}
\begin{aligned}
t\_{c}\:\:=\:\:t\_{seq}\times{\frac{w\_c}{problem\_{size}}}
\end{aligned}
\end{equation}

Where $t\_{seq}$ is the to run the whole amount of work, $problem\_{size}$ sequentially.

\begin{equation}\label{formula42}
\begin{aligned}
execution\_time = 
\alpha\times{\left\lceil{\frac{num\_{tasks}}{N}}\right\rceil}\:\:&+\:\:t\_{seq}\times{\frac{w\_c}{problem\_{size}}}\times{(1\:\:+\:\:\gamma\times{(M-1)})}\:\:\\
&+\:\:t\_{overhead\_{ws}}\\
\end{aligned}
\end{equation}

In order to test the proposed model we created a benchmark based on a simple \textit{for\textunderscore{loop}} with different number of iterations, iteration lengths, and chunk sizes, as shown in Listing~\ref{hpx_for_loop}. 
Each iteration consists of a while loop that makes sure the iteration lasts a certain amount of time($iter\_{length}$). By changing $iter\_{length}$, $num\_{iterations}$, and $chunk\_{size}$, we can see how the execution time changes when the problem is executed on different number of cores. Here, we define \textit{problem\textunderscore{size}} as the time it takes to execute all the iterations, which is:

\begin{equation}\label{problem_size}
problem\_size = iter\_length\times{num\_iterations}
\end{equation}

Figure~\ref{fig40} highlights the effect of work stealing for $problem\_size=3000$ on 4 and 8 threads. As it can be observed, in the region where the number of tasks created is same as the number of cores, this effect is more significant. Although we are not sure how work stealing is causing this effect, \textcolor{red}{we can use a term with Normal distribution with $\mu=\frac{ps}{N}$. We believe this term is originated from HPX's implementation of work stealing, and although it affects the execution time at certain points, since it only adds to the execution time at those points, and our interest is finding the flat region of the graph with the minimum execution time, at this point and for this purpose we could ignore this term in our upcoming calculations.}  
By ignoring the overhead caused by work stealing at this point, Formula~\ref{formula1} is simplified into Formula~\ref{formula7}, which will be referred to as our proposed analytical model in the next sections. 

\begin{equation}\label{formula7}
\begin{aligned}
execution\_time = 
\alpha\times{\left\lceil{\frac{num\_{tasks}}{N}}\right\rceil}\:\:&+\:\:t\_c\:\:+\:\:\gamma\times{t\_c}\times{(M-1)}
\end{aligned}
\end{equation}

\vspace{\baselineskip}	
\begin{figure}[H]
	\centering
	\subfloat[]{\includegraphics[scale=.4]{images/hpx_for_loop/3000_4_1_all.png}\label{fig40:a}}{\hfill}
	\subfloat[]{\includegraphics[scale=.4]{images/hpx_for_loop/3000_8_1_all.png}\label{fig40:b}}
	\caption{The results of running the benchmark in Listing with $problem\_size=3000$, on different (a)4 cores, and (b) 8 cores. The vertical dotted line shows the grain size that would generate same number of tasks as the number of cores, with the same amount of work for all the cores.}\label{fig40}		
\end{figure}
 
 
 
Figure~\ref{fig39} shows an example of the results obtained from running the benchmark, for $problem\_size=10000$, on different number of cores.

\vspace{\baselineskip}	
\begin{figure}[H]
	\centering
	{\includegraphics[scale=.45]{images/hpx_for_loop/10000_8_all.png}}
	\caption{The results of running the benchmark in Listing with $problem\_size=10000$, on different number of cores.}\label{fig39}		
\end{figure}

As stated in Chapter~\ref{Background}, at the right hand side of the graph in Figure~\ref{fig39}, the number of tasks created is smaller than the number of cores which results in making at least one of the cores idle, while the other cores are assigned a rather big chunk of work. The performance degradation we observe in that points is associated with starvation, meaning that we are not utilizing our computation resources to the full extent. In these points, the number of cores actually doing the work is equal to the number of the tasks, since each core gets to execute at most one task. At this region of the graph, the time to execute the maximum assigned work to a core ($t\_c$) is the dominant factor.   

On the other hand, on the left hand side of the graph, since the grain size is very small we end up with creating a large number of tasks. Since there is an overhead associated with each created task, we observe a performance degradation in that region. As the grain size increases, the number of created tasks, and the overhead associated to that decreases consequently. At this region of the graph, the overhead of creating and managing the tasks is the dominant factor.   

To summarize, for this simple experiment, where we do not have to deal with the cash effects, we believe the important factors are: number of HPX threads being created, number of cores the program is ran on, the maximum amount of work one core has to perform. 

The maximum number of tasks assigned to one core, and the number of cores that are actually performing the work, are two other important factors that can be deducted from the aforementioned factors. 


\vspace{\baselineskip}
\subsection{Validating the Proposed Model}
In order to test how the well the suggested model in Formula~\ref{formula7} could fit the data, we collected a considerable amount of data with different configurations of number of cores($N$), number of iterations(\emph{num\_{iterations}}), and chunk size($chunk\_{size}$). We set $iter\_{length}$ to $1\mu\text{sec}$ for simplification. For each $num\_{iterations}$, we change the $chunk\_{size}$ from 1 to $num\_{iterations}$ in logarithmic scale. Each of these runs was executed for $N=1,2,3,...,8$.  

As an example, for $problem\_{size}=100000$, we have collected 64 data points for each $N=1,2,...,8$, resulting in $512$ data points in total. Figure~\ref{fig41} shows the execution time for different grain sizes ranging from 1 to 100000 for $problem\_{size}=100,000$ with $N=8$.



\begin{figure}[H]
	\centering
	{\includegraphics[scale=.45]{images/hpx_for_loop/100000_8.png}}
	\caption{The results of running the benchmark in Listing with $problem\_size=100,000$, on 8 cores. The unit for execution time is microseconds.}\label{fig41}		
\end{figure}


Since our proposed model does not depend on the $problem\_{size}$ but on the number of tasks created, number of cores, and the time to execute the maximum amount of work assigned to a core, we are suggesting here that it might be sufficient to use the data collected from one $problem\_{size}$ to find the parameters $\alpha$ and $\gamma$. 

In order to test the viability of our proposition, we chose $problem\_{size}=100,000,000$ as a reasonable(not too small nor too large) base for our parameter estimation. Figure~\ref{fig42} shows the measured execution time in terms of grain size for $N=1,2,...,8$, with total $728$ points.
  
\begin{figure}[H]
	\centering
	{\includegraphics[scale=.45]{images/hpx_for_loop/100000000_8.png}}
	\caption{The results of running the benchmark in Listing with $problem\_size=100,000,000$, on 8 cores. The unit for execution time is microseconds.}\label{fig42}		
\end{figure}

Using all the $728$ data points for different number of cores, we used the $optimize.curve_fit$ package from $scipy$ library in Python to fit the collected data with our model, which resulted in $alpha=3.032$ and $\gamma=0.294$. $\alpha$ represents the overhead of creating associated with creating tasks in $\mu{secs}$, and $\gamma$ represents the contention.

\begin{figure}[H]
	\centering
	\subfloat[]
	{\centering\includegraphics[scale=.4]{images/hpx_for_loop/fitted/100000000_1.png}	
		\label{fig43:a}}
	\subfloat[]
	{\centering\includegraphics[scale=.4]{images/hpx_for_loop/fitted/100000000_2.png}	
	\label{fig43:b}}\hfill
	\subfloat[]
{\centering\includegraphics[scale=.4]{images/hpx_for_loop/fitted/100000000_3.png}	
	\label{fig43:c}}
\subfloat[]
{\centering\includegraphics[scale=.4]{images/hpx_for_loop/fitted/100000000_4.png}	
	\label{fig43:d}}\hfill
	\subfloat[]
{\centering\includegraphics[scale=.4]{images/hpx_for_loop/fitted/100000000_5.png}	
	\label{fig43:e}}
\subfloat[]
{\centering\includegraphics[scale=.4]{images/hpx_for_loop/fitted/100000000_6.png}	
	\label{fig43:f}}\hfill
	\subfloat[]
{\centering\includegraphics[scale=.4]{images/hpx_for_loop/fitted/100000000_7.png}	
	\label{fig43:g}}
\subfloat[]
{\centering\includegraphics[scale=.4]{images/hpx_for_loop/fitted/100000000_8.png}	
	\label{fig43:h}}\hfill
	\caption{The results of predicted values of execution time through curve fitting vs the real data for with $problem\_size=100,000,000$, for (a) 1 core, (b) 2 cores, (c) 3 cores, (d) 4 cores, (e) 5 cores, (f) 6 cores, (g) 7 cores, (h) 8 cores.. The unit for execution time is microseconds.}
	\label{fig43}	
\end{figure}


Figure~\ref{fig43} shows the fitted curves along with the original data for different number of cores.
In order to measure how well the model is fitting the data we calculated the relative error as Formula~\ref{eq4}, where $p_i$ is the predicted value from curve fitting for sample $i$, $t_i$ is true value of that sample, and $n$ is the total number of samples.


\begin{equation}{\label{eq4}}
Relative\_{error} = \frac{1}{n}\sum_{i=1}^{n} {|1-\frac{p_i}{t_i}|}
\end{equation}

\begin{figure}[H]
	\centering
	{\includegraphics[scale=.45]{images/hpx_for_loop/fitted/marvin_relative_error_100000000.png}}
	\caption{The relative error of using the result of curve fitting for predicting the execution, for different number of cores, for $problem\_size=100,000$.}\label{fig44}		
\end{figure}



\begin{figure}[H]
	\centering
	{\includegraphics[scale=.45]{images/hpx_for_loop/fitted/marvin_relative_error_all.png}}
	\caption{The average relative error of using the result of curve fitting for $problem\_size=100,000,000$ for predicting the execution of different $problem\_{sizes}$ for different number of cores.}\label{fig45}		
\end{figure}

\begin{equation}{\label{eq5}}
R\textunderscore{squared} = 1-\frac{{\frac{1}{n}\sum_{i=1}^{n}{(t_i-p_i)}^2}}{Var(t)}
\end{equation}

\begin{figure}[H]
	\centering
	{\includegraphics[scale=.45]{images/hpx_for_loop/fitted/marvin_r2_error_all.png}}
	\caption{The average $R^2$ score of using the result of curve fitting for $problem\_size=100,000,000$ for predicting the execution of different $problem\_{sizes}$ for different number of cores.}\label{fig46}		
\end{figure}

At the next step we repeated the same process for the data collected from a different architecture, \textit{Medusa} node.
We used $problem\_{size}=1,000,000$, and after curve fitting the parameters $\alpha=1.793$ and $\gamma=0.279$ were obtained. The calculated mean relative error and mean $R^2$ measure for each individual number of cores is demonstrated in Figure~\ref{fig48}, and Figure~\ref{fig49}.

\begin{figure}[H]
	\centering
	\subfloat[]
	{\centering\includegraphics[scale=.4]{images/hpx_for_loop/fitted/medusa_1000000_1.png}	
		\label{fig50:a}}
	\subfloat[]
	{\centering\includegraphics[scale=.4]{images/hpx_for_loop/fitted/medusa_1000000_2.png}	
		\label{fig50:b}}\hfill
	\subfloat[]
	{\centering\includegraphics[scale=.4]{images/hpx_for_loop/fitted/medusa_1000000_3.png}	
		\label{fig50:c}}
	\subfloat[]
	{\centering\includegraphics[scale=.4]{images/hpx_for_loop/fitted/medusa_1000000_4.png}	
		\label{fig50:d}}\hfill
	\subfloat[]
	{\centering\includegraphics[scale=.4]{images/hpx_for_loop/fitted/medusa_1000000_5.png}	
		\label{fig50:e}}
	\subfloat[]
	{\centering\includegraphics[scale=.4]{images/hpx_for_loop/fitted/medusa_1000000_6.png}	
		\label{fig50:f}}\hfill
	\subfloat[]
	{\centering\includegraphics[scale=.4]{images/hpx_for_loop/fitted/medusa_1000000_7.png}	
		\label{fig50:g}}
	\subfloat[]
	{\centering\includegraphics[scale=.4]{images/hpx_for_loop/fitted/medusa_1000000_8.png}	
		\label{fig50:h}}\hfill
	\caption{The results of predicted values of execution time through curve fitting vs the real data for with $problem\_size=1,000,000$, for (a) 1 core, (b) 2 cores, (c) 3 cores, (d) 4 cores, (e) 5 cores, (f) 6 cores, (g) 7 cores, (h) 8 cores. The unit for execution time is microseconds.}
	\label{fig50}	
\end{figure}

\begin{figure}[H]
	\centering
	{\includegraphics[scale=.45]{images/hpx_for_loop/fitted/medusa_relative_error_100000.png}}
	\caption{The relative error of using the result of curve fitting for predicting the execution, for different number of cores, for $problem\_size=100,000$, on \textit{Medusa} node.}\label{fig47}		
\end{figure}



\begin{figure}[H]
	\centering
	{\includegraphics[scale=.45]{images/hpx_for_loop/fitted/medusa_relative_error_all.png}}
	\caption{The average relative error of using the result of curve fitting for $problem\_size=100,000,000$ for predicting the execution of different $problem\_{sizes}$ for different number of cores, on \textit{Medusa} node.}\label{fig48}		
\end{figure}

\begin{figure}[H]
	\centering
	{\includegraphics[scale=.45]{images/hpx_for_loop/fitted/medusa_r2_error_all.png}}
	\caption{The average $R^2$ score of using the result of curve fitting for $problem\_size=100,000,000$ for predicting the execution of different $problem\_{sizes}$ for different number of cores, on \textit{Medusa} node.}\label{fig49}		
\end{figure}


\subsection{Generalizing the Proposed Analytical Model}
In the previous section we proposed an analytical model to predict the execution time based on grain size for a simple for loop benchmark. In this section we aim to generalize the proposed model from the simple for loop in Listing~\ref{hpx_for_loop} in which the work executed at each iteration is actually keeping that thread busy for a certain amount of time, into loops that are performing some work. 

To  


\subsection{Applying the Proposed Model to the Blazemark Data}

%\vspace{\baselineskip}	
%\begin{table}[H]
%	\centering
%	\resizebox{\textwidth}{!}
%	{\begin{tabular}{|c | c |} 
%			\hline
%			Category & Configuration \\
%			\hline
%			\hline
%		$Problem\_{size}$ & 100000 \\ [0.5ex] 
%			\hline
%			$N$ & 1, 2, 3, 4, 5, 6, 7, 8 \\ 	
%			\hline
%			$num\_{iterations}$ & 100000 \\
%			\hline	
%			$chunk\_{size}$ & 1,2,3,4,5,6,7,8,9,10,20,30,...,90000,100000 \\
%			\hline		
%	\end{tabular}}
%	
%	\caption{List of different configurations used for running the benchmark in Listing for $problem\_{size}=100000$}
%	\label{table6}
%\end{table}
   



\vspace{\baselineskip}
\subsection{Analyzing the Data}
Assuming the proposed formula is a good fit for this problem, we can estimate the range of grain sizes for which we achieve the lowest execution time, for a specific problem size, run on specific number of cores.

\vspace{\baselineskip}
\subsubsection{Left side of the graph}
As stated earlier, in Formula~\ref{formula1}, for small grain sizes the first term is the dominant factor while the second term roughly stays constant. Same way, for large grain sizes the second factor is the dominant factor. 

In order to find the lower-bound of the range for which the execution time stays constant, we assume the second factor is constant in that region. Also we can change $N$ to $M$, knowing that our concern is on the left hand side of the graph, where $num\_{tasks}$ is definitely greater than the number of cores. 
Taking the derivative of the function based on the grain size then leads to:

 
\begin{equation}\label{formula3}
\begin{aligned}
\frac{\partial execution\_{time}}{\partial g} &= \frac{\alpha}{N}\times{\frac{\partial num\_{tasks}}{\partial g}}+\delta\times{\frac{\partial num\_{tasks}}{\partial g}} \\
&=(\frac{\alpha}{N}+\delta)\times\frac{\partial(\frac{problem\_{size}}{g})}{\partial g} \\
&=(\frac{\alpha}{N}+\delta)\times{problem\_{size}}\times{\frac{-1}{g^2}}
\end{aligned}
\end{equation}

From Formula~\ref{formula3}, it can be observed that for the left hand side of the graph the rate of changes is negative and decreases as the grain size increases. Here we are looking for the value of the grain size for which the rate of change becomes very small (we introduce a threshold $\lambda_b$, where $\lambda_b\ll1$, for this purpose). 


\begin{equation}\label{formula4}
\begin{aligned}
&\frac{\alpha}{N}\times{problem\_{size}}\times{\frac{1}{g^2}}\leq{\lambda_b} \\
{g^2}&\geq{\frac{(\frac{\alpha}{N}+\delta)\times{problem\_{size}}}{\lambda_b}}\\
{g}&\geq{\sqrt{\frac{\frac{\alpha}{N}\times{problem\_{size}}}{\lambda_b}}}
\end{aligned}
\end{equation}

Formula~\ref{formula4} can also be represented as shown in Formula~\ref{formula5}. This representation shows that when the ratio of the time it takes to execute one task to the total overhead of creating and managing $num\_{tasks}$ tasks on $N$ core, is greater than a threshold, we will end up in the flat region of the graph, close to the left hand side.


\begin{equation}\label{formula5}
\begin{aligned}
\frac{\alpha}{N}\times{\frac{problem\_{size}}{g}}\times{\frac{1}{g}}&\leq{\lambda_b}\\
\frac{\alpha}{N}\times{num\_{tasks}}\times{\frac{1}{g}}&\leq{\lambda_b}\\
\frac{\alpha}{N}\times{num\_{tasks}}&\leq{g\times\lambda_b}\\	
\frac{g}{\frac{\alpha}{N}\times{num\_{tasks}}}&\geq{\frac{1}{\lambda_b}}
\end{aligned}
\end{equation}


\subsubsection{Right side of the graph}
Now looking at the right hand side of the graph, the overhead of creating the tasks becomes negligible on that side, since only few tasks are being created and the overhead of creating these many tasks is not significant compared to the execution time. On this side, the maximum amount of work executed by one core ($w\_c$) and consequently the time to perform this amount of work ($t\_c$) is the dominant factor.

Formula~\ref{formula2} shows how $w\_c$ is calculated for different cases, but in general we can estimate $w\_c$ with $g\times\left \lceil{\frac{num\_{tasks}}{N}}\right \rceil$. 

\begin{equation}\label{formula6}
\begin{aligned}
w\_c&\approx{g\times\left \lceil{\frac{num\_{tasks}}{N}}\right \rceil}\\
&\approx{g\times\left \lceil{\frac{\frac{problem\_{size}}{g}}{N}}\right \rceil}\\
&\approx{\frac{problem\_{size}}{N}} \:\:\text{\:\:\:\:if\:\:} num\_{tasks}\geq{N}
\end{aligned}
\end{equation}

What happens here is that as the grain size changes, there are points for which $\left \lceil{\frac{num\_{tasks}}{N}}\right \rceil$ is the same but since the grain size is different, a different $w\_c$ would be resulted. 
For all the values of $g$ that create the same $\left \lceil{\frac{num\_{tasks}}{N}}\right \rceil$, as $g$ increases the difference between $w\_c$ and $\frac{problem\_{size}}{N}$ increases. 

For example, considering a case where $problem\_{size}=100,000$, and $N=8$, for the grain sizes in range of $[4167,\:6249]$ would result in creating between $\left \lceil{\frac{100,000}{6,249}}\right \rceil=17$ and  $\left \lceil{\frac{100,000}{4,167}}\right \rceil=24$ tasks. This amount of tasks created itself would result in $\left \lceil{\frac{17}{8}}\right \rceil=3$ and  $\left \lceil{\frac{24}{8}}\right \rceil=3$ tasks.
On the other hand, $w\_c=g\times{\left \lceil{\frac{num\_{tasks}}{N}}\right \rceil}=3\times{g}$, would have a value in range of $[3\times4167,\:3\times6249]=[12501,\: 18747]$, where the average amount of work per core is $\frac{problem\_{size}}{N}=12500$. This means that for grain sizes closer to the end of the range, we are observing that a much bigger amount of work is assigned to the core with maximum amount of work, which would result in a higher execution time. 


In the general case, if we denote $\left \lceil{\frac{num\_{tasks}}{N}}\right \rceil$ as $k$, then:


\begin{equation}\label{formula8}
\begin{aligned}
k-1&<{\frac{num\_{tasks}}{N}}\leq{k}\\
(k-1)\times{N}&<num\_{tasks}\leq{k}\times{N}\\
%num\_{tasks}=\left \lceil{\frac{problem\_{size}}{g}}\right \rceil\\
(k-1)\times{N}&<\left \lceil{\frac{problem\_{size}}{g}}\right \rceil\leq{k}\times{N}\\
(k-1)\times{N}&<\frac{problem\_{size}}{g}\leq{{k}\times{N}}\\
\end{aligned}
\end{equation}

If $k=1$, then, 

\begin{equation}\label{formula9}
\begin{aligned}
0<\frac{problem\_{size}}{g}\leq{N}\\
\frac{problem\_{size}}{N}\leq{g}\leq{problem\_{size}}.
\end{aligned}
\end{equation}

Otherwise, when $k>1$,

\begin{equation}\label{formula10}
\begin{aligned}
\frac{problem\_{size}}{k\times{N}}\leq{g}<\frac{problem\_{size}}{(k-1)\times{N}}.
\end{aligned}
\end{equation}

Since $\left \lceil{\frac{num\_{tasks}}{N}}\right \rceil=k$, and $w\_c={g\times\left \lceil{\frac{num\_{tasks}}{N}}\right \rceil}={k\times{g}}$ if $num\_{tasks}\%{N}\neq{1}$, we can conclude for $k>1$:

\begin{equation}\label{formula11}
\begin{aligned}
k\times{\frac{problem\_{size}}{k\times{N}}}&\leq{w\_c}<{k\times{\frac{problem\_{size}}{(k-1)\times{N}}}}\\
\frac{problem\_{size}}{{N}}&\leq{w\_c}<{\frac{k}{k-1}\times{\frac{problem\_{size}}{N}}}\\
0&\leq{w\_c-\frac{problem\_{size}}{N}}<\frac{1}{k-1}\times{\frac{problem\_{size}}{N}}
\end{aligned}
\end{equation}

For the cases where $k>1$ and $num\_{tasks}\%{N}={1}$, there could be a  change in $w\_c$ if $problem\_{size}\%{g}\neq{0}$. For these cases:


\begin{equation}\label{formula15}
\begin{aligned}
\left \lceil{\frac{num\_{tasks}}{N}}\right \rceil&=k \text{\:\:\:}\&\text{\:\:\:} num\_{tasks}\%{N}={1}\Rightarrow\\
num\_{tasks}&=(k-1)\times{N}+1\Rightarrow\\
(k-1)\times{N}&<{\frac{problem\_{size}}{g}}\leq(k-1)\times{N}+1\Rightarrow\\
\frac{problem\_{size}}{(k-1)\times{N}+1}&\leq{g}<\frac{problem\_{size}}{(k-1)\times{N}}
\end{aligned}
\end{equation}

From Formula~\ref{formula2} we know,
\begin{equation}\label{formula17}
\begin{aligned}
w\_c={problem\_{size}}-(k-1)\times(N-1)\times{g}.
\end{aligned}
\end{equation}

Therefore,
\begin{equation}\label{formula18}
\begin{aligned}
(k-1)(N-1)\frac{problem\_{size}}{(k-1){N}+1}&\leq{(k-1)(N-1){g}}<(k-1)(N-1)\frac{problem\_{size}}{(k-1){N}}\Rightarrow\\
{\frac{problem\_{size}}{N}}&<{problem\_{size}-{(k-1)(N-1){g}}}\leq{k\frac{problem\_{size}}{(k-1){N}+1}}\\
{\frac{problem\_{size}}{N}}&<{w\_{c}}\leq{k\times\frac{problem\_{size}}{(k-1){N}+1}}
\end{aligned}
\end{equation}

And for $k=1$, where $num\_{tasks}\leq{N}$,
\begin{align*}\label{formula12}
w\_c&=g\\
\frac{problem\_{size}}{N}&\leq{g}\leq{problem\_{size}}\Rightarrow
\end{align*}

\begin{equation}\label{formula14}
\begin{aligned}
0\leq{w\_c-\frac{problem\_{size}}{N}}={g-\frac{problem\_{size}}{N}}\leq(N-1)\times{\frac{problem\_{size}}{N}}\\
\end{aligned}
\end{equation}


Defining $imbalance\_{ratio}=\frac{w\_c-\frac{problem\_{size}}{N}}{\frac{problem\_{size}}{N}}$, then,

\begin{equation}\label{formula13}
\begin{aligned}
0&\leq{imbalance\_{ratio}}\leq{N-1}  \text{\:\:\:\:\:\:\:\:\:\:\:\:\:for\:\:\:}k={1}\\
0&\leq{imbalance\_{ratio}}<\frac{1}{k-1}  \text{\:\:\:\:\:\:\:\:\:\:\:\:\:for\:\:\:}k>{1} \text{\:\:and\:\:}num\_{tasks}\%{N}\neq{1}\\
0&\leq{imbalance\_{ratio}}<\frac{N-1}{N(k-1)+1}=\frac{1}{k-1+\frac{k}{N-1}}\text{\:\:\:\:\:\:\:\:\:\:\:\:\:otherwise}
\end{aligned}
\end{equation}

%k\in\mathbb{Z}
Formula~\ref{formula13} shows that as number of created tasks increases, as long as number of tasks per core is the same, the imbalance factor decreases. 

\vspace{\baselineskip}

Figure~\ref{fig38} shows the imbalance ratio calculated for different grain sizes for $problem\_size=10000$, on 8 cores. Each of the regions between two dashed green lines correspond to a specific value for $k=\left\lceil{\frac{num\_{tasks}}{N}}\right \rceil$. 
At each of the regions with $k>1$, $\left\lceil{\frac{num\_{tasks}}{N}}\right \rceil=k$,  $imbalance\_{ratio}$ starts from $0$ and approaches $\frac{1}{k-1}$ ($\frac{1}{k-1+\frac{k}{N-1}}$ for regions where $num\_{tasks}\%{N}\neq{1}$) at the end of the region. When $k=1$, $imbalance\_{ratio}$ increases linearly starting from 0 and reaching the maximum of $N-1$ when $g=pronblem\_{size}$. As we move to larger grain sizes, $\left\lceil{\frac{num\_{tasks}}{N}}\right \rceil$ decreases, therefore the upper-bound for $imbalance\_{ratio}$ increases.   


\vspace{\baselineskip}	
\begin{figure}[H]
	\centering
	{\includegraphics[scale=.25]{images/hpx_for_loop/w_c_all.png}}
	\caption{The imbalance ratio calculated for different grain sizes for $problem\_size=10000$, on 8 cores, where $k=\left\lceil{\frac{num\_{tasks}}{N}}\right \rceil$.}\label{fig38}		
\end{figure}


%\vspace{\baselineskip}	
%\begin{figure}[H]
%	\centering
%	{\includegraphics[scale=.45]{images/hpx_for_loop/max_w_c.png}}
%	\caption{The maximum imbalance ratio calculated for different grain sizes for $problem\_size=10000$, on 8 cores, where $k=\left\lceil{\frac{num\_{tasks}}{N}}\right \rceil$.}\label{fig36}		
%\end{figure}

Figure~\ref{fig39} represents the imbalance ratio, along with the ratio of the sequential execution time over execution time(speed-up) against grain size for $problem\_size=10000$, ran on 8 cores.   
As it can be observed, as the $imbalance\_{ratio}$ increases, the speed-up decreases. 

\vspace{\baselineskip}	
\begin{figure}[H]
	\centering
	{\includegraphics[scale=.3]{images/hpx_for_loop/w_c_speedup.png}}
	\caption{The imbalance ratio calculated for different grain sizes for $problem\_size=10000$, on 8 cores, where $k=\left\lceil{\frac{num\_{tasks}}{N}}\right \rceil$.}\label{fig37}		
\end{figure}


To summarize, as the grain size increases the maximum imbalance in the loads assigned to the cores also increases, and some point on, this imbalance has a significant affect in the execution time. We define a threshold, $\lambda_s$ ($0<\lambda_s<1$), where for $imbalance\_{ratio}$s smaller than this threshold the imbalance effect is not significant. As we get close to this threshold, we are likely to reach the right hand side of the flat region of the bathtub curve of the execution time against grain size. 


We are interested in finding the maximum grain size that would generate a reasonable imbalance ($imbalance\_{ratio}\leq{\lambda_s}$), to make sure we should stay in the flat region of the bathtub curve of execution time against grain size, from load imbalance point of view.  

Formula~\ref{formula14} states that for grain sizes greater than $problem\_{size}$, $imblance\_{ratio}$ increases linearly with grain size from $0$ to $N-1$. While for grain sizes smaller than $problem\_{size}$, the maximum $imbalance\_{ratio}$ depends on $k=\left\lceil{\frac{num\_{tasks}}{N}}\right\rceil$. So, in order to assure \emph{imbalance\_{ratio}} is smaller than or equal to a threshold ($\lambda_s$), first we search the grain sizes smaller than $\frac{problem\_{size}}{N}$. Since $0<\lambda_s<1$, and $k\geq2$ in this region, there exists a $k$ such that $\frac{1}{k-1}\leq\lambda_s$.    
If there exists a $k_{min}$ (creating an imbalance ratio between $0$ and $\frac{1}{k_{min}-1}$), where $\frac{1}{k_{min}-1}\leq{\lambda_s}$, $\forall k<k_{min}$ maximum value of $imbalance\_{ratio}$ would be greater than $\lambda_s$. So in order to find the grain size that would create maximum $imbalance\_{ratio}$ of $\lambda_s$:



\begin{equation}\label{formula21}
\begin{aligned}
&imbalance\_{ratio}\leq{{\lambda_s}}\Rightarrow\\
&\frac{1}{k-1}\leq\lambda_s\\
&k\geq{1+\frac{1}{\lambda_s}}\\
&k_{min}=\left\lceil{1+\frac{1}{\lambda_s}}\right\rceil+1\\
&{g}<\frac{problem\_{size}}{(k_{min}-1)\times{N}}\\
&g_{max}=\frac{problem\_{size}}{(k_{min}-1)\times{N}}-1=\frac{problem\_{size}}{(1+\left\lceil{\frac{1}{\lambda_s}}\right\rceil)\times{N}}
\end{aligned}
\end{equation}

If $g<g\_{max}$, we can assure that $imbalance\_{ratio}$ never exceeds $\lambda_s$. Since we already found a match at grain sizes smaller than $\frac{problem\_{size}}{N}$, checking the rest of grain sizes would not be necessary.





\begin{lstlisting}[basicstyle=\fontsize{8}{9}\selectfont,float,floatplacement=H,caption= {A simple hpx for\textunderscore{loop} used to study the effect of grain size on the achieved parallelism.}, label={hpx_for_loop}]

///////////////////////////////////////////////////////////////////////////////
void measure_function_futures_for_loop(std::uint64_t count, bool csv, std::uint64_t chunk_size, std::uint64_t iter_length)
{
// start the clock
high_resolution_timer walltime;
hpx::parallel::for_loop(hpx::parallel::execution::par.with(
hpx::parallel::execution::dynamic_chunk_size( chunk_size )),
0, count, [&](std::uint64_t) { worker_timed(iter_length*1000); });
// stop the clock
const double duration = walltime.elapsed();
print_stats("for_loop", "par", "parallel_executor", count, duration, csv);
}

///////////////////////////////////////////////////////////////////////////////
int hpx_main(variables_map& vm)
{
{
const int repetitions = vm["repetitions"].as<int>();
num_threads = hpx::get_num_worker_threads();
const std::uint64_t chunk_size = vm["chunk_size"].as<std::uint64_t>();
const std::uint64_t iter_length = vm["iter_length"].as<std::uint64_t>();
const std::uint64_t count = vm["num_iterations"].as<std::uint64_t>();
bool csv = vm.count("csv") != 0;
if (HPX_UNLIKELY(0 == count))
throw std::logic_error("error: count of 0 futures specified\n");
for (int i = 0; i < repetitions; i++)
{
measure_function_futures_for_loop(count, csv, chunk_size, iter_length);
}
}
return hpx::finalize();
}
///////////////////////////////////////////////////////////////////////////////
inline void worker_timed(std::uint64_t delay_ns)
{
if (delay_ns == 0)
return;
std::uint64_t start = hpx::util::high_resolution_clock::now();
while (true)
{
// Check if we've reached the specified delay.
if ((hpx::util::high_resolution_clock::now() - start) >= delay_ns)
break;
}
}
///////////////////////////////////////////////////////////////////////////////
int main(int argc, char* argv[])
{
// Configure application-specific options.
options_description cmdline("usage: " HPX_APPLICATION_STRING " [options]");
cmdline.add_options()("num_iterations",
value<std::uint64_t>()->default_value(500000),
"number of iterations to invoke")
("repetitions", value<int>()->default_value(1),
"number of repetitions of the full benchmark")
("iter_length",value<std::uint64_t>()->default_value(1), "length of each iteration")
("chunk_size",value<std::uint64_t>()->default_value(1), "chunk size");
// Initialize and run HPX.
return init(cmdline, argc, argv);
}
\end{lstlisting}

\vspace{\baselineskip}
\subsection{Identifying the range of grain size for maximum speedup}
In the previous section, we proposed a method to identify the lower-bound and the upper-bound of the grain sizes for which we observe the minimum execution time. 
Integrating Formula~\ref{formula4} and Formula~\ref{formula21} suggests the following range for maximum speedup:


\begin{equation}\label{formula23}
\begin{aligned}
{\sqrt{\frac{(\frac{\alpha}{N}+\delta)\times{problem\_{size}}}{{\lambda_b}}}}\leq{g}\leq\frac{problem\_{size}}{(1+\left\lceil{\frac{1}{\lambda_s}}\right\rceil)\times{N}}
\end{aligned}
\end{equation}
 
Where $0\leq\lambda_s\leq1$, and $\lambda_b,\lambda_s\ll1$.

In this section we represent the identified range for a number of problem sizes, with different number of cores.


%\vspace{\baselineskip}
%\subsection{Analyzing the Data}
%Throughout these experiments we try to fix or eliminate the factors we believe will affect the execution time as much as possible, study the results, and then introduce one factor to the model and study the behavior of the model. 



\vspace{\baselineskip}
%\subsection{Step 1: Sequential Run}
%In the first step in order to eliminate all the factors related to parallel execution we run the code sequentially for some problem sizes. The obtained result, as shown in Figure~\ref{fig26} demonstrates an overhead that increases as the number of iterations increases.
%
%\begin{figure}
%	\centering
%	\includegraphics[width=1\linewidth]{images/hpx_for_loop/overheads_seq.png}
%	\caption{The results obtained from running the hpx for loop sequentially}	
%	\label{fig26}
%\end{figure}




\vspace{\baselineskip}

\clearpage

\subsection{Step 1: Single-task, single-core runs}
At the first step, we look at the cases where only one task has been created, the program is run on only one core. This problem could be assumed to be equivalent to running the same amount of work sequentially with an additional cost of creating just one task. 
\subsubsection{Expected Model}
With this assumption we expect the execution time to be summation of the time it takes to perform the total amount of work(\textit{problem\textunderscore{size}}) and the overhead of creating one HPX task($\alpha$). Formula~\ref{chunk1} shows the expected formula.

\begin{equation}\label{chunk1}
execution\:\:time = \alpha + problem\:\:size
\end{equation}  

\vspace{\baselineskip}
\subsubsection{Original Data}
In order to check our proposed model for this simplified problem, we collected data from running the program, setting the \textit{chunk\textunderscore{size}} to 1, \textit{num\textunderscore{iterations}} to 1, and changing the \textit{iter\textunderscore{length}} from 1 to 10,000,000. 




%To summarize, knowing the grain size, we are expecting the execution time in a many-task runtime system to be mainly affected by these factors, the overhead of creating one task($\alpha$), the number of cores that are actually doing the work($M$), the sequential execution time($t_s$), and finally the portion of the program that could actually be parallelized($\gamma$). 
%
%If we try to integrate these information into a formula, we would expect the relation between execution time($t$) and number of tasks($n_t$) as follows:
%\begin{equation}\label{new1}
%t=\frac{\alpha{n_t}+t_s}{M}+\gamma
%\end{equation}
%
%which could be decomposed into these two equations:
%
%
%Now we use this function to find the best three parameters $\alpha$, $t_s$, and $\gamma$ so that the collected data would fit this model. For this purpose we used the \textit{curve\textunderscore{fit}} package from \textit{SciPy} library in \textit{python}.
%
%In order to make Equation~\ref{new1} differentiable, we used the softplus function(Equation~\ref{softplus1}) to represent $M$ based on $n_t$.
%\begin{equation}\label{softplus1}
%f(x)=Ln(1+e^x)
%\end{equation}
%
%Which results in Equation~\ref{soft_plus1}:
%\begin{equation}\label{soft_plus1}
%t=\frac{\alpha{n_t}+t_s}{(N-1)-Ln(1+(e^{N-1}-1)e^{-n_t})}+\gamma
%\end{equation}




\vspace{\baselineskip}