Linear algebra libraries like ATLAS, SPIRAL,... try to use hardware-specific
optimizations to improve their performance. In this work, we are trying to
optimize the performance based on the application parameters such as matrix
size, operation, and data layout.   

Scientific applications tend to contain a lot of task parallelism, performing
same set of operations on different chunks of the data. Using a parallel
for-loop for this purpose can lead to significant speed-ups.

Defining chunk as the group of iterations that would be assigned to a processor,
loop scheduling methods propose different approaches for creation and assignment
of these chunks to the processors.


Loop scheduling refers to different ways iterations could be assigned to the
processors and the order of their execution. The main reason for performance
degradation in loop scheduling is load imbalance, which refers to situations
where different amount of work is assigned to different
processors\cite{ciorba2018openmp}.    

The simplest loop scheduling method is static scheduling, in which, the
iterations are divided evenly among all the processors statically, either as a
consecutive block -also called cyclic- or in a round-robin
manner\cite{liu1994safe}. Since all the assignments happen at compile time or
before execution of the application, this method imposes no runtime scheduling
overhead. Several factors including interprocessor communication, cache misses,
and page faults can lead to different execution times for different iterations,
leading to load imbalance among the processors\cite{philip1995increasing}.

In the meanwhile, dynamic scheduling methods postpone the assignment to runtime,
which tends to improve load balancing, at the cost of higher scheduling
overhead. Some of dynamic scheduling methods include: Pure Self-scheduling,
Chunk Self-scheduling, Guided Self-scheduling\cite{polychronopoulos1987guided},
Factoring\cite{hummel1992factoring} and Trapazoid
Self-scheduling\cite{tzen1993trapezoid},\cite{liu1994safe}.We briefly go over
some of these loop scheduling techniques here.


In Pure Self-scheduling everytime a processors becomes idle, it fetches one loop
iteration. This approach, while achieving a high load balance, imposes a
considerable amount of scheduling overhead when we are dealing with a fine-grain
workload, and a large number of iterations. Also frequent access to shared
variables like loop index could lead to memory contention\cite{liu1994safe}. 

In order to decrease the high scheduling overhead of Pure Self-scheduling
methods, Chunk Self-scheduling method assigns a certain number of
iterations(called chunk size) to each idle processor. This method trades lower
scheduling overhead with higher load imbalance. Selection of the chunk size
plays a very important role in the performance, as so a large chunk size
increases the scheduling overhead decreases and causes load imbalance, while a
small chunk size increases memory contention and scheduling
overhead\cite{liu1994safe}. 

As an adaptive loop scheduling technique, Guided
Self-scheduling\cite{polychronopoulos1987guided} divides the remaining number of
iterations at each request evenly among the processors, and assigns it to the
processor that made the request, while updating the number of remaining
iterations. This causes larger number of iterations to be assigned to the
processors at the beginning of the loop execution, which results in lower
scheduling overhead. The number of iterations assigned to each processor
decreases as it approaches to the end of the execution, generating tasks
containing only one or two iterations, causing an increase in the scheduling
overhead. In order to tackle this issue, a minimum number of chunks could be set
to avoid creation of very small chunks\cite{lilja1994exploiting}. 

Very similar to Guided Self-scheduling, Factoring also decreases the chunk size
as the loop execution proceeds, with this difference that   

-dynamic
-self-scheduling
-factoring

talk about load balancing and work stealing

But each of these methods work well for specific problem. We are looking for a
general solution which can automatically decide on the chunk size parameter to
achieve the best performance.

\vspace{\baselineskip}
\section{Problem Statement}
Importance of compile time configuration on task scheduling


\vspace{\baselineskip}
