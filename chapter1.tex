The current microprocessors are able to deliver a peak performance in range of hundreds of Mflops to Gflops. But in order to achieve these performances a lot of effort needs to be made to optimize your program based on the architecture it would be run on\cite{whaley1998automatically}. 

The core element of many high performance computing applications is the linear algebra library. linear algebra libraries like ATLAS, SPIRAL,... try to use hardware-specific optimizations to improve their performance. In this work, we are trying to optimize the performance based on the application parameters such as matrix size, operation, the expression, data layout, and also the machine architecture.   


\vspace{\baselineskip}
\section{Thesis Statement}
The main objective of this thesis is to propose a hybrid runtime and compile-time solution for a linear algebra library to fully take advantage of the available parallelism and resources. 

We chose Blaze math library since it is a nice high performance template-based C++ library that allows you to access the expression tree for each assignment at compile time, and we chose HPX as an asynchronous may-task runtime system to manage the parallelism. 

HPX makes it possible to create thousands to millions of lightweight user threads, to avoid expensive context switching. On the other hand, although the overhead of creating one task is negligible, creating millions of tasks when the execution time of the program itself is small, could become significant and cause performance degradation. On the other hand if we create too few tasks it would be very likely for us to not use our resources properly. So in very application in many task systems, it's very important to chose the amount of work assigned to each task, called grain size, properly. 

Through analyzing and modeling the relationship between throughput and grain size, we would be able to identify a range of grain size that leads us to maximum performance. Once decided how big one unit of work should be, based on the identified range we would be able to decide on how many units of work should be packed into one task.

On the other hand, there are different models to express the relationship between the throughput and the number of cores. Here, we are interested in developing a model to be as realistic as possible to imitate the behavior of the throughput against both grain size and number of cores. This would help us find how to manage the parallelism in our system to achieve the highest performance possible.
 
\vspace{\baselineskip}
\section{Contributions}
There has been a wide study, mostly by Gunther\cite{gunther2000practical,gunther2002new,gunther2007guerrilla,gunther2011new}, on different models to represent the relationship between the throughput and the number of cores, for a fixed size problem. 
Grubel et.al\cite{grubel2015performance} has studied the effect of task granularity on the performance with a fixed number of cores. 
Our contributions could be summarized into:
\begin{itemize}
	\item{We propose a novel physical model to represent how the execution time is expected to change based on grain size.}
	\item{To our knowledge, there has not been a work to create a 3D model of the throughput, grain size, and number of cores.} 
	\item{We are proposing a method to apply the developed model to a linear algebra library, in a way specific to our application, and the machine architecture.}
\end{itemize}
 

\section{Document Organization}
In Chapter~\ref{Background}, we will explain briefly the background needed for this thesis, including Blaze and HPX library, the effect of task granularity, and the Universal Scaling Law(USL) method for modeling the throughput based on number of cores.  
Chapter~\ref{Literature} refers to other works that have been done in our area of our focus. We explain our proposed method to optimize the performance, along with the models we used in Chapter~\ref{Method}. Our work heavily relies on the collected data, the environment we collected the data from and also the library versions are mentioned in Chapter~\ref{Results}.
Finally we discuss our concerns and the further steps that needs to be taken in Chapter~\ref{Future}.

\vspace{\baselineskip}
